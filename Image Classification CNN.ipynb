{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Setting the working directory\n",
    "os.chdir(r'C:\\Users\\Atul\\Desktop\\python files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\aravi\\\\Desktop\\\\python files\\\\data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d3fbb08a115d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Firstly we make the folder path.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\aravi\\Desktop\\python files\\data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\aravi\\Desktop\\python files\\data\\train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\aravi\\Desktop\\python files\\data\\test\\test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\aravi\\\\Desktop\\\\python files\\\\data'"
     ]
    }
   ],
   "source": [
    "#Firstly we make the folder path.\n",
    "os.makedirs(r\"C:\\Users\\Atul\\Desktop\\python files\\data\")\n",
    "os.makedirs(r\"C:\\Users\\Atul\\Desktop\\python files\\data\\train\")\n",
    "os.makedirs(r\"C:\\Users\\Atul\\Desktop\\python files\\data\\test\\test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c88d3179eceb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_image.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/test/test/test_image.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_image.jpg'"
     ]
    }
   ],
   "source": [
    "src = r\"C:\\Users\\Atul\\Desktop\\Jupyter\\10_categories\"\n",
    "dest = r\"C:\\Users\\Atul\\Desktop\\Jupyter\\data\\train\\train\"\n",
    "copyDirectory(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    " \n",
    "def copyDirectory(src, dest):\n",
    "    try:\n",
    "        shutil.copytree(src, dest)\n",
    "    # Directories are the same\n",
    "    except shutil.Error as e:\n",
    "        print('Directory not copied. Error: %s' % e)\n",
    "    # Any error saying that the directory doesn't exist\n",
    "    except OSError as e:\n",
    "        print('Directory not copied. Error: %s' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = r\"C:\\Users\\Atul\\Desktop\\Jupyter\\10_categories\"\n",
    "dest = r\"C:\\Users\\Atul\\Desktop\\Jupyter\\data\\train\\train\"\n",
    "copyDirectory(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs(r\"C:\\Users\\Atul\\Desktop\\Jupyter\\data\\train\\validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder does already exits!\n",
      "Folder does already exits!\n",
      "Folder does already exits!\n",
      "Folder does already exits!\n",
      "Folder does already exits!\n",
      "Folder does already exits!\n",
      "Folder does already exits!\n",
      "Folder does already exits!\n",
      "Folder does already exits!\n",
      "Folder does already exits!\n",
      "Folder does already exits!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "inputpath = r\"C:\\Users\\Atul\\Desktop\\python files\\data\\train\\train\"\n",
    "outputpath = r\"C:\\Users\\Atul\\Desktop\\python files\\data\\train\\validation\"\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(inputpath):\n",
    "    word =dirpath[len(inputpath):]\n",
    "    \n",
    "    structure = outputpath +word\n",
    "    if not os.path.isdir(structure):\n",
    "        os.mkdir(structure)\n",
    "    else:\n",
    "        print(\"Folder does already exits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplanes\n",
      "278\n",
      "522\n",
      "BACKGROUND_Google\n",
      "156\n",
      "312\n",
      "bonsai\n",
      "47\n",
      "81\n",
      "car_side\n",
      "51\n",
      "72\n",
      "Faces\n",
      "135\n",
      "300\n",
      "Faces_easy\n",
      "144\n",
      "291\n",
      "grand_piano\n",
      "37\n",
      "62\n",
      "Leopards\n",
      "71\n",
      "129\n",
      "Motorbikes\n",
      "251\n",
      "547\n",
      "watch\n",
      "128\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir = 'C:/Users/Atul/Desktop/python files/data'\n",
    "\n",
    "trn = base_dir + \"/train/train/\"\n",
    "dirName = os.listdir(trn)\n",
    "\n",
    "for x in dirName:\n",
    "    print(x)\n",
    "    sourceN = base_dir + \"/train/train/\"+ x +\"/\"\n",
    "    destN = base_dir + \"/train/validation/\"+ x\n",
    "\n",
    "    filesN = os.listdir(sourceN)\n",
    "    \n",
    "    for f in filesN:\n",
    "        if np.random.rand(1) < 0.2:\n",
    "            shutil.move(sourceN + '/'+ f, destN + '/'+ f)\n",
    "        \n",
    "    print(len(os.listdir(sourceN)))\n",
    "    print(len(os.listdir(destN)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest') # 'nearest' is kind of algorithm to fill pixel values while transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get VGG16 architecture from keras.applications\n",
    "from keras.applications.vgg16 import VGG16,GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "# downloading weights and convolution layers from vgg16\n",
    "trained_model= VGG16(include_top=False,weights='imagenet') # include_top = False means it removes the MLP part of the N/W. So we get only the convolution part \n",
    "\n",
    "# defining mlp that needs to be appended to vgg16 convolution layers\n",
    "x = trained_model.output\n",
    "x = GlobalAveragePooling2D()(x) # we dont have a window size.. Do it on the whole thing\n",
    "x = Dense(512,activation='relu')(x)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256,activation='relu')(x)\n",
    "\n",
    "x = Dropout(0.4)(x)\n",
    "output = Dense(10,activation='softmax')(x) # We can use either 2 neurons with softmax or one neuron with sigmoid\n",
    "model= Model(inputs=trained_model.input,outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer in trained_model.layers:\n",
    "    layer.trainable=False \n",
    "    \n",
    "#compiling\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 15,111,242\n",
      "Trainable params: 396,554\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and validation generators\n",
    "This is extremely useful when the samples are arranged in a structure, where all samples corresponding to one class are in a single folder. And all such folders corresponding to train/validation are in a parent folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1297 images belonging to 10 classes.\n",
      "Found 2427 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# This is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# This is the augmentation configuration we will use for testing:\n",
    "# Only rescaling. Other transformations are not required for testing. Duh!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(  \n",
    "        'data/train/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "# save to directory can also be given as an argument here\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/train/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 19s 6s/step - loss: 0.9326 - acc: 0.6875 - val_loss: 0.7286 - val_acc: 0.7812\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.8872 - acc: 0.7083 - val_loss: 0.6497 - val_acc: 0.8125\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.6731 - acc: 0.7917 - val_loss: 0.6446 - val_acc: 0.7812\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.7122 - acc: 0.7604 - val_loss: 0.6378 - val_acc: 0.8125\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.7005 - acc: 0.7812 - val_loss: 0.5986 - val_acc: 0.8438\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.5362 - acc: 0.8438 - val_loss: 0.5009 - val_acc: 0.8438\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.6051 - acc: 0.8333 - val_loss: 0.4638 - val_acc: 0.8750\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.6447 - acc: 0.8125 - val_loss: 0.4463 - val_acc: 0.9062\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.4546 - acc: 0.9062 - val_loss: 0.4737 - val_acc: 0.8438\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.3345 - acc: 0.9167 - val_loss: 0.5373 - val_acc: 0.7812\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.4361 - acc: 0.8333 - val_loss: 0.4330 - val_acc: 0.8125\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.4197 - acc: 0.9062 - val_loss: 0.3522 - val_acc: 0.8438\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.3294 - acc: 0.9386 - val_loss: 0.3512 - val_acc: 0.9375\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 19s 6s/step - loss: 0.3694 - acc: 0.8854 - val_loss: 0.4527 - val_acc: 0.8125\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.3782 - acc: 0.8750 - val_loss: 0.4390 - val_acc: 0.7812\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.3195 - acc: 0.9375 - val_loss: 0.3628 - val_acc: 0.8125\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2773 - acc: 0.9375 - val_loss: 0.2972 - val_acc: 0.9062\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.2428 - acc: 0.9455 - val_loss: 0.2366 - val_acc: 0.9688\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.3760 - acc: 0.8646 - val_loss: 0.2450 - val_acc: 0.9688\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.2231 - acc: 0.9167 - val_loss: 0.2677 - val_acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=120//batch_size, # '//' in python returns only the quotient\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=45//batch_size)\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BACKGROUND_Google': 0, 'Faces': 1, 'Faces_easy': 2, 'Leopards': 3, 'Motorbikes': 4, 'airplanes': 5, 'bonsai': 6, 'car_side': 7, 'grand_piano': 8, 'watch': 9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_generator.class_indices) # This returns class labels of directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame.from_dict(train_generator.class_indices, orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aravi\\\\Desktop\\\\python files'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'data/test/',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[[0.7539706  0.00230637 0.00888526 0.00203927 0.02210028 0.00179307\n",
      "  0.00929745 0.00113137 0.13414292 0.06433338]\n",
      " [0.7539706  0.00230637 0.00888526 0.00203927 0.02210028 0.00179307\n",
      "  0.00929745 0.00113137 0.13414292 0.06433338]]\n"
     ]
    }
   ],
   "source": [
    "test_prob = model.predict_generator(test_generator, steps=2) # this returns the probabilities\n",
    "test_pred_classes = np.argmax(test_prob, axis=1) # convert probabilities to classes\n",
    "print(test_pred_classes)\n",
    "print(test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test\\\\test_image.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Check the corresponding filenames of the predictions\n",
    "print(test_generator.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BACKGROUND_Google'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class Name for final\n",
    "df.index[test_pred_classes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
